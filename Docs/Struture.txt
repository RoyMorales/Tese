
Struture:
  
  Introduction:
    Transformer
      1.4.1) Attention (Geral introduction -> Spcial Attention and self-attention focused)
      1.4.2) Architeture overlook
      1.4.3) Tranformer applied to Vision (Vit)
      1.4.4) Scalability

    State of Art:


    Results:
      -> NoPE (no position)
      -> RoPE (state of art for llms)

    Conclucion









